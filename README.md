# Neural Network with Custom Layer Class

## Description
This project demonstrates a basic implementation of a neural network using custom layer classes in Python with NumPy.
The network includes forward and backward propagation capabilities, utilizing sigmoid and ReLU activation functions.
The example provided trains a network on randomly generated data and labels.

## Installation
To run the code, you'll need Python and NumPy installed. You can install NumPy using pip:
## Implementation Details
The network consists of several fully connected layers, defined by the layer class.
Each layer performs the following:
Initialization: Sets up weights and biases with random values.
Forward Propagation: Computes the output using ReLU for hidden layers and sigmoid for the output layer.
Backward Propagation: Updates weights and biases using gradients calculated based on the error.

## Contributions
Contributions are welcome! Please open an issue or submit a pull request for any changes or suggestions.
## Acknowledgements
This implementation is inspired by foundational concepts in neural networks and deep learning. Special thanks to the broader community for resources and guidance in understanding backpropagation and activation functions.

